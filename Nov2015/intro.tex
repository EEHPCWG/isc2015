\section{Introduction}
\label{intro}

Current Supercomputing Centers (SCs) for High-Performance Computing (HPC) with peta-scale capabilities have high power demands, with peak requirements of over 30 MW and fluctuations of a few megawatts. This trend is expected to continue in the future as we push the limits of supercomputing further. As a result, Electricity Service Providers (ESPs) for such SCs need to support efficient electricity generation, transmission and distribution along with reliable grid operation. ESPs today already face reliability concerns for accommodating megawatt-level fluctuations from SCs and often require HPC client sites to forecast their electricity use. The acceptance and proliferation of renewable sources of energy further adds to the variability in electricity generation, making grid reliability even more challenging. A tighter integration and open communication between ESPs and their client SCs is thus critical as we proceed toward the next generation of supercomputing. 

At present, most ESP-SC relationships are linear and unidirectional. Power is typically generated, distributed and delivered to customer sites without direct or active involvement, and most electricity pricing contracts are negotiated without any communication requirements. Going forward, however, it is expected that a multi-directional relationship will evolve between the ESPs and SCs.  Communication and control will flow from end-customers to one or more of the electricity generation and distribution entities, and contract terms will enforce stringent usage requirements. The cloud and data center providers, such as Google, have already started to anticipate this multi-directional relationship and are taking advantage of this changing landscape.  For example, Google's response suggests vertical integration, especially with Google's Energy Subsidiary which gives Google the right to sell energy within the United States~\cite{GoogleAdv}. Another example is the SmartGrid initiative \cite{SmartGrid} by the U.S. Department of Energy, which is making electricity delivery faster and more efficient by involving customers, adjusting to dynamic demands, and by providing automated solutions and quick responses to remote facilities. \emph{Demand management} (DM) is a set of explicit actions taken by large-scale data centers, cloud providers, SCs and other entities in order to establish such multi-directional relationships with their ESPs. One key element is the \emph{temporal} component that indicates the timescale requirements for the DM actions. The benefits of DM depend on the timescales of negotiation and implementation of this relationship with the ESPs. The Energy-Efficient High-Performance Computing Working Group (EE HPC WG) seeks to analyze the impact of DM implementations for SCs with HPC workloads and for their ESPs. 

In our previous work, we focused on understanding how ESPs and SCs can work together to improve DM through grid-integrated services by surveying large-scale SCs in the United States \cite{BatesESP}. We developed a questionnaire and surveyed 11 sites. We noted that none of the SCs are working directly with their ESPs to leverage the benefits of DM. Our main conclusion from this work was that SCs in the United States were interested in a tighter integration with their ESPs, but a business case for the same had not been well-demonstrated. In this work, we expand our analysis to include European SCs. We accomplish this by extending the aforementioned questionnaire and quantitatively surveying nine European SCs. 

%TP:Should be part of results.
%Nine out of the sixteen SCs that we contacted responded to the questionnaire. All except one of these sites were in Top 50 supercomputers in the world \cite{Top500}. \\

The main motivation for our geographical study lies in the way electricity is priced. In Europe, electricity is more expensive and is subject to more variability because of the larger mix of renewable sources. Additionally, the SCs in both geographical regions have different maximum power demands. For example, in the United States, four of the SCs we surveyed had HPC workloads of 10 MW or more. The remaining SCs in the United States as well as all the SCs in Europe had workloads of 5 MW of less. The size of demand and its variability have different and co-related impacts on the operation of the SCs and grid. 

Our initial expectation was that that the European SCs will be more tightly integrated with their ESPs because of the higher prices and more extensive use of renewables.  Contrary to our expectations, however, we found that the United States shows more interest in responding to requests from their ESPs than Europe. The four SCs that needed 10MW or more had active communication channels with their ESPs about responding to grid requests.  None of the SCs in Europe had similar relationships with their ESPs. In this paper, we present these results and analyze the differences across the two geographies that may have led to this result. We first present results from our quantitative survey from 9 European SC sites and 11 United States SC sites, and then conduct a detailed qualitative analysis for three major SCs: Oak Ridge National Laboratory (ORNL), Lawrence Livermore National Laboratory (LLNL), and Leibniz Supercomputing Center (LRZ). The key goal of the qualitative analysis is to delve deeper into the electricity pricing structures as well as the available incentives for a tighter integration and to understand what motivates the existing relationship between SCs and their ESPs. 

Section \ref{strategies} presents an overview of DM actions and motivates the need for an open multi-directional relationship between SCs and their ESPs. Section \ref{res} discusses the quantitative results from the questionnaire. In Section \ref{comm}, we highlight some of the comments we received from our respondents.  Section \ref{intws} discuss the details of our site-specific interviews and present a qualitative analysis of the demand management options available to these sites. Section \ref{relwork} presents related work, and Section \ref{summary} concludes this article.